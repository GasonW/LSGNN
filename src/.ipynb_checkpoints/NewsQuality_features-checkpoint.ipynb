{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "from __future__ import division\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import ast\n",
    "from datetime import timedelta\n",
    "from pyhanlp import *\n",
    "import re\n",
    "from preprocess import load_stopwords\n",
    "from getSentimentFromTencent import get_content\n",
    "from gen_21dim_vector import get_emotion_value\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Hand-Crafted Features\"\"\"\n",
    "def load_features_name():\n",
    "    feature_list = []\n",
    "    f = open(\"../model/manual_feature.txt\", \"r\")\n",
    "    for line in f:\n",
    "        feature_list.append(line.strip())\n",
    "    f.close()\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hanlp_segment(all_data):\n",
    "\n",
    "    all_data['content'] = all_data['weibo_content'].fillna('_##_')\n",
    "    all_data['seg'] = None\n",
    "    all_data['pos'] = None\n",
    "    for weiboId in all_data.index:\n",
    "        content = all_data.loc[weiboId, 'weibo_content']\n",
    "        \"\"\"Clean sentences\"\"\"\n",
    "        try:\n",
    "            content = content.replace(' ', '')  \n",
    "            content = content.replace('/', '') \n",
    "        except:\n",
    "            break\n",
    "        \"\"\"Segmentation\"\"\"\n",
    "        after_seg = HanLP.segment(content)\n",
    "        segs = []\n",
    "        poses = []\n",
    "        for term in after_seg:\n",
    "            segs.append(term.word)\n",
    "            poses.append(str(term.nature))\n",
    "                \n",
    "        seg_str = \" \".join(segs)\n",
    "        pos_str = \" \".join(poses)\n",
    "        all_data.loc[weiboId, 'seg'] = seg_str\n",
    "        all_data.loc[weiboId, 'pos'] = pos_str\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_faces(content):\n",
    "    \"\"\"calculate the number of emojis\"\"\"\n",
    "    try:\n",
    "        co = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "    except re.error:\n",
    "        co = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "    n_emoji = len(co.findall(content))\n",
    "    return n_emoji\n",
    "\n",
    "def cal_urls(content):\n",
    "    \"\"\"calculate the number of urls\"\"\"\n",
    "    co = re.compile(u'http')\n",
    "    n_url = len(co.findall(content))\n",
    "    has_url = 1 if n_url > 0 else 0\n",
    "    return has_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(all_data):\n",
    "\n",
    "    first_pronoun = ['我', '我们']\n",
    "    second_pronoun = ['你', '你们']\n",
    "    third_pronoun = ['他', '他们', '她', '她们', '它', '它们']\n",
    "    adversative = ['但', '但是', '然而', '却', '而', '偏偏', '只是', '不过', '至于', '致', '不料', '岂知', '可是']\n",
    "    function_pos = ['d', 'p', 'c', 'u', 'e', 'o']\n",
    "    question_mark = ['？','?']\n",
    "    exclamation_mark = ['！','!']\n",
    "    broken_point = ['，', '；', '、', '：', ',', ';', ':']\n",
    "    sentence_end = ['】', '。', '？', '！', '...','.', '?', '!', ']']\n",
    "    modal_particle = ['难道','决','岂','反正','也许','大约','大概','果然','居然','竟然','究竟','幸而','幸亏','偏偏','明明','恰恰','未免','只好','不妨','索性','简直','就','可','难怪','反倒','何尝','何必']\n",
    "    adv_of_degree = ['很','非常','极','十分','最','顶','太','更','挺','极其','格外','分外','更加','越','越发','有点儿','稍','稍微','略微','几乎','过于','尤其']\n",
    "    official_speech = ['通报', '称']\n",
    "    uncertain_words = ['可能','也许','似乎','大概','或许']\n",
    "    forward_reference = ['他', '他们', '她', '她们', '它', '它们','那','那些','这', '这些']\n",
    "    professional_words_pos = ['g', 'gb', 'gbc', 'gc','gg','gi', 'gm', 'gp']\n",
    "\n",
    "    high_features = list(['interactivity', 'interestingness', 'moving', 'persuasive', 'logic', 'readability', 'formality','Integrity1'])\n",
    "    text_features = list(set(load_features_name()).difference(set(high_features)))\n",
    "\n",
    "    new_data = pd.concat([all_data, pd.DataFrame(columns=text_features)])\n",
    "    new_data.index.name = 'weiboId'\n",
    "    for weiboId in all_data.index:\n",
    "\n",
    "        feature_temp = dict.fromkeys(text_features, 0)\n",
    "        feature_temp['sentences'] = 1\n",
    "        original_content = new_data.loc[weiboId, 'weibo_content']\n",
    "        feature_temp['face_num'] = cal_faces(original_content)\n",
    "        feature_temp['hasUrl'] = cal_urls(original_content)\n",
    "\n",
    "        segs = new_data.loc[weiboId, 'seg']\n",
    "        poses = new_data.loc[weiboId, 'pos']\n",
    "        segs = segs.split(' ')\n",
    "        poses = poses.split(' ')\n",
    "        \n",
    "        feature_temp['sentiment_score'] = get_emotion_value(segs)\n",
    "        \n",
    "        for pos in poses:\n",
    "            if pos == 'm':\n",
    "                feature_temp['numerals'] += 1\n",
    "            elif pos in function_pos:\n",
    "                feature_temp['function_words'] += 1\n",
    "            elif pos == 't':\n",
    "                feature_temp['time_num'] += 1\n",
    "            elif 'ns' in pos:\n",
    "                feature_temp['place_num'] += 1\n",
    "            elif 'nr' in pos:\n",
    "                feature_temp['object_num'] += 1\n",
    "            elif pos == 'a':\n",
    "                feature_temp['adj_num'] += 1\n",
    "            elif pos == 'y':\n",
    "                feature_temp['modal_particle_num'] += 1\n",
    "            elif pos == 'cc':\n",
    "                feature_temp['conj_num'] += 1\n",
    "            elif 'ry' in pos:\n",
    "                feature_temp['Interrogative_pron_num'] += 1\n",
    "            elif pos == 'i':\n",
    "                feature_temp['idiom_num'] += 1\n",
    "            elif re.match('p', pos) != None:\n",
    "                feature_temp['prep_num'] += 1\n",
    "            elif re.match('v', pos) != None:\n",
    "                feature_temp['verb_num'] += 1\n",
    "            elif re.match('d', pos) != None:\n",
    "                feature_temp['adv_num'] += 1\n",
    "            if pos in professional_words_pos:\n",
    "                feature_temp['professional_words_num'] += 1\n",
    "            if re.match('n', pos) != None:\n",
    "                feature_temp['noun_num'] += 1\n",
    "            if re.match('r', pos) != None:\n",
    "                feature_temp['pron_num'] += 1\n",
    "        characters = 0\n",
    "        words = 0\n",
    "        broken_nums = 0\n",
    "        LW = 0\n",
    "\n",
    "        for word in segs:\n",
    "            characters = characters + len(word)\n",
    "            words = words + 1\n",
    "            if word == '#':\n",
    "                feature_temp['tags'] += 1\n",
    "                feature_temp['hasTag'] = 1\n",
    "            elif word == '@':\n",
    "                feature_temp['@'] += 1\n",
    "                feature_temp['hasAt'] = 1\n",
    "            elif word in exclamation_mark:\n",
    "                feature_temp['exclamation_mark_num'] += 1\n",
    "            elif word in question_mark:\n",
    "                feature_temp['question_mark_num'] += 1\n",
    "            elif word in first_pronoun:\n",
    "                feature_temp['first_pronoun_num'] += 1\n",
    "            elif word in second_pronoun:\n",
    "                feature_temp['second_pronoun_num'] += 1\n",
    "            elif word in third_pronoun:\n",
    "                feature_temp['third_pronoun_num'] += 1\n",
    "            elif word in adversative:\n",
    "                feature_temp['adversative_num'] += 1\n",
    "            elif word in modal_particle:\n",
    "                feature_temp['modal_particle_num'] += 1\n",
    "            elif word == '”' or word == '“':\n",
    "                feature_temp['rhetoric_num'] += 1\n",
    "            elif word in adv_of_degree:\n",
    "                feature_temp['adv_of_degree_num'] += 1\n",
    "            elif word in official_speech:\n",
    "                feature_temp['official_speech_num'] += 1\n",
    "            elif word in uncertain_words:\n",
    "                feature_temp['uncertainty'] += 1\n",
    "            elif word == '【':\n",
    "                feature_temp['hasHead'] = 1\n",
    "            if word in forward_reference:\n",
    "                feature_temp['forward_reference_num'] += 1\n",
    "            if len(word) > 2:\n",
    "                LW += 1\n",
    "            if word in sentence_end:\n",
    "                feature_temp['sentences'] += 1\n",
    "            elif word in broken_point:\n",
    "                broken_nums += 1\n",
    "\n",
    "        feature_temp['LW'] = LW\n",
    "        feature_temp['RIX'] = LW/feature_temp['sentences']\n",
    "        feature_temp['LIX'] = words/feature_temp['sentences'] + (100*LW)/words\n",
    "\n",
    "        feature_temp['rhetoric_num'] = feature_temp['rhetoric_num'] / 2\n",
    "        feature_temp['sub_sentences'] = broken_nums + feature_temp['sentences']\n",
    "        feature_temp['sentence_broken'] = feature_temp['sub_sentences'] / feature_temp['sentences']\n",
    "        feature_temp['characters'] = characters\n",
    "        feature_temp['words'] = words\n",
    "        feature_temp['average_word_length'] = characters / words\n",
    "\n",
    "        for fea in feature_temp.keys():\n",
    "            new_data.loc[weiboId, fea] = feature_temp[fea]\n",
    "        print(weiboIdId,'basic feature finished')\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(all_data,text_features):\n",
    "    all_data[text_features] = preprocessing.scale(all_data[text_features])\n",
    "    return all_data\n",
    "\n",
    "def normlization(all_data, text_features):\n",
    "    all_data[text_features] = preprocessing.normalize(all_data[text_features], norm='l2')\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_high_features(all_data):\n",
    "    \"\"\"Calculate high level features\"\"\"\n",
    "    high_features = list(['interactivity', 'interestingness', 'moving', 'persuasive', 'logic', 'readability', 'formality','Integrity1'])\n",
    "    data = pd.concat([all_data, pd.DataFrame(columns=high_features)])\n",
    "\n",
    "    for Id in data.index:\n",
    "        data.loc[Id, 'interactivity'] = data.loc[Id,'question_mark_num'] + data.loc[Id,'first_pronoun_num'] + data.loc[Id,'second_pronoun_num'] + data.loc[Id,'Interrogative_pron_num']\n",
    "        data.loc[Id,'interestingness'] = data.loc[Id,'rhetoric_num'] + data.loc[Id,'exclamation_mark_num'] + data.loc[Id,'face_num'] + data.loc[Id,'adj_num'] + data.loc[Id,'idiom_num'] + data.loc[Id,'adversative_num']\n",
    "        data.loc[Id,'moving'] = data.loc[Id,'sentiment_score'] + data.loc[Id,'first_pronoun_num'] + data.loc[Id,'second_pronoun_num'] + data.loc[Id,'exclamation_mark_num'] + data.loc[Id,'question_mark_num'] + data.loc[Id,'adv_of_degree_num'] + data.loc[Id,'modal_particle_num']\n",
    "        data.loc[Id,'persuasive'] = data.loc[Id,'numerals'] + data.loc[Id,'@'] + data.loc[Id,'official_speech_num'] + data.loc[Id,'time_num'] + data.loc[Id,'place_num'] + data.loc[Id,'object_num'] - data.loc[Id,'uncertainty']\n",
    "        data.loc[Id,'logic'] = data.loc[Id,'forward_reference_num'] + data.loc[Id,'conj_num']\n",
    "        data.loc[Id,'readability'] = -(data.loc[Id,'sentence_broken'] + data.loc[Id,'characters'] + data.loc[Id,'words'] + data.loc[Id,'average_word_length'] + data.loc[Id,'sentences'] + data.loc[Id,'sub_sentences'] + data.loc[Id,'professional_words_num'] + data.loc[Id,'RIX'] + data.loc[Id,'LIX'] + data.loc[Id,'LW'])\n",
    "        data.loc[Id,'formality'] = data.loc[Id,'noun_num'] + data.loc[Id,'adj_num'] + data.loc[Id,'prep_num'] - data.loc[Id,'pron_num'] - data.loc[Id,'verb_num'] - data.loc[Id,'adv_num'] - data.loc[Id,'sentence_broken']\n",
    "        data.loc[Id,'Integrity1'] = 2 * data.loc[Id,'hasHead'] + 2 * data.loc[Id,'hasTag'] + data.loc[Id,'hasAt'] + data.loc[Id,'hasUrl']\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'val'\n",
    "data = pd.read_csv('../data/ictmcg_' + data_type + '.csv', header=0)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation\n",
    "data_new = hanlp_segment(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hand-crafted features\n",
    "data_new = feature(data_new)\n",
    "data_new.to_csv('../data/ictmcg_' + data_type + '_NQ.csv', index=None, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "high_features = list(['interactivity', 'interestingness', 'moving', 'persuasive', 'logic', 'readability', 'formality','Integrity1'])\n",
    "text_features = list(set(load_features_name()).difference(set(high_features)))\n",
    "\n",
    "data_new = scale(data_new,text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate high level features\n",
    "data_new = cal_high_features(data_new)\n",
    "\n",
    "# Normalization\n",
    "data_new = scale(data_new, high_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.to_csv('../data/ictmcg_' + data_type + '_NQ_scale.csv', index=None, sep=',', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
